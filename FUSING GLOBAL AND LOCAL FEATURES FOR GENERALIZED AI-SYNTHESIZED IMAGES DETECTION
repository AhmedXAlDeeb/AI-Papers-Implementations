{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":6122531,"sourceType":"datasetVersion","datasetId":3509534}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim\nfrom torchvision import datasets, transforms, models\nimport torch.optim as optim\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport shutil\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"2a1a2e8b-5068-4ebc-b0a0-5588fd3ee6c7","_cell_guid":"c5def81d-8f16-45ce-b459-8309a6dba6a8","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:22.484353Z","iopub.execute_input":"2024-07-24T10:11:22.484771Z","iopub.status.idle":"2024-07-24T10:11:22.490203Z","shell.execute_reply.started":"2024-07-24T10:11:22.484743Z","shell.execute_reply":"2024-07-24T10:11:22.489392Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f'Using device: {device}')","metadata":{"_uuid":"99627c32-65e8-4206-b451-51564bbb6b9a","_cell_guid":"a934d08d-7bb4-4c76-aabd-262101219db1","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:25.317164Z","iopub.execute_input":"2024-07-24T10:11:25.317496Z","iopub.status.idle":"2024-07-24T10:11:25.322233Z","shell.execute_reply.started":"2024-07-24T10:11:25.317468Z","shell.execute_reply":"2024-07-24T10:11:25.321481Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real_images_path = '/kaggle/input/fake-or-real-dataset/train/real_images'\nfake_images_path = '/kaggle/input/fake-or-real-dataset/train/fake_images'\nreal_images = os.listdir(real_images_path)\nfake_images = os.listdir(fake_images_path)\nreal_train, real_temp = train_test_split(real_images, train_size=50, random_state=42)\nfake_train, fake_temp = train_test_split(fake_images, train_size=50, random_state=42)\nreal_val, real_test = train_test_split(real_temp, test_size=10, random_state=42)\nreal_val = real_val[:100]  # Limit to 100 samples\n\nfake_val, fake_test = train_test_split(fake_temp, test_size=10, random_state=42)\nfake_val = fake_val[:100]  # Limit to 100 samples","metadata":{"_uuid":"14196dc3-7599-42af-b77e-bd061a82992a","_cell_guid":"08cc3323-65e3-49cf-9aed-7c525cbc72d1","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:35.113777Z","iopub.execute_input":"2024-07-24T10:11:35.114153Z","iopub.status.idle":"2024-07-24T10:11:36.780340Z","shell.execute_reply.started":"2024-07-24T10:11:35.114123Z","shell.execute_reply":"2024-07-24T10:11:36.779499Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchSelectionModule(nn.Module):\n    def __init__(self, patch_size=8, num_patches=0.75):\n        super(PatchSelectionModule, self).__init__()\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n    def calculate_patch_scores(self, activation_map):\n        # Get dimensions\n        batch_size, _, height, width = activation_map.size()\n        scores = F.avg_pool2d(activation_map, self.patch_size, stride=2)\n        return scores\n\n    def non_max_suppression(self, coords, scores, threshold=0.5):\n        sorted_indices = torch.argsort(scores, descending=True)\n        sorted_scores = scores[sorted_indices]\n        coords = torch.tensor(coords)\n        sorted_coords = coords[sorted_indices]\n\n        keep_indices = []\n        while len(sorted_coords) > 0:\n            current = sorted_coords[0]\n            keep_indices.append(sorted_indices[0])\n            sorted_coords = sorted_coords[1:]\n            sorted_indices = sorted_indices[1:]\n            if len(sorted_coords) == 0:\n                break\n            ious = self.iou(current, sorted_coords)\n            keep = ious < threshold\n            sorted_coords = sorted_coords[keep]\n            sorted_indices = sorted_indices[keep]\n        return coords[keep_indices], scores[keep_indices]\n\n    def iou(self, boxA, boxesB):\n        xA1, yA1 = boxA[0]\n        xA2, yA2 = boxA[1]\n        xB1, yB1 = boxesB[:, 0, 0], boxesB[:, 0, 1]\n        xB2, yB2 = boxesB[:, 1, 0], boxesB[:, 1, 1]\n        inter_x1 = torch.max(xA1, xB1)\n        inter_y1 = torch.max(yA1, yB1)\n        inter_x2 = torch.min(xA2, xB2)\n        inter_y2 = torch.min(yA2, yB2)\n        interArea = torch.max(torch.tensor(0), inter_x2 - inter_x1) * torch.max(torch.tensor(0), inter_y2 - inter_y1)\n        boxAArea = (xA2 - xA1) * (yA2 - yA1)\n        boxBArea = (xB2 - xB1) * (yB2 - yB1)\n        iou = interArea / (boxAArea + boxBArea - interArea)\n        return iou\n\n    def calc_topk_coords(self, scores, threshold=0.5):\n        flattened_scores = scores.view(scores.size(0), -1)\n        num_patches = min(self.num_patches * flattened_scores.shape[1], flattened_scores.shape[1])\n        topk_scores, topk_indices = torch.topk(flattened_scores, int(num_patches), dim=1)\n        topk_coords = []\n        for idx in range(scores.size(0)):\n            coords = []\n            for index in topk_indices[idx]:\n                x, y = divmod(index.item(), scores.size(2))\n                coords.append((x, y))\n            topk_coords.append(coords)\n        return topk_scores, topk_coords\n\n    def get_patch_coordinates(self, activation_map, original_image_size):\n        scores = self.calculate_patch_scores(activation_map)\n        topk_scores, topk_coords = self.calc_topk_coords(scores)\n        scale_h = original_image_size[0] / activation_map.size(2)\n        scale_w = original_image_size[1] / activation_map.size(3)\n        patch_coords = []\n        for coords in topk_coords:\n            image_coords = []\n            for (x, y) in coords:\n                top_left = (int(y * scale_w), int(x * scale_h))\n                bottom_right = (int((y + self.patch_size) * scale_w), int((x + self.patch_size) * scale_h))\n                image_coords.append((top_left, bottom_right))\n            patch_coords.append(image_coords)\n        return patch_coords\n\n    def get_patches(self, patch_coordinates, original_image):\n        patches = []\n        for coords in patch_coordinates:\n            image_patches = []\n            for (top_left, bottom_right) in coords:\n                patch = original_image[:, top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n                image_patches.append(patch)\n            patches.append(torch.stack(image_patches))\n        patches = torch.stack(patches)\n        return patches\n\n    def forward(self, activation_map, original_image_size):\n        activation_map = activation_map.sum(dim=1).unsqueeze(1)\n        patch_coordinates = self.get_patch_coordinates(activation_map, original_image_size)\n        return patch_coordinates\n\nclass GlobalBranch(nn.Module):\n    def __init__(self):\n        super(GlobalBranch, self).__init__()\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(2048, 128)\n\n    def forward(self, x):\n        F = self.resnet(x)\n        pooled_F = self.avg_pool(F)\n        pooled_F = pooled_F.view(pooled_F.size(0), -1)\n        global_embeddings = self.fc(pooled_F)\n        return F, global_embeddings\n\nclass LocalEmbeddingExtractor(nn.Module):\n    def __init__(self):\n        super(LocalEmbeddingExtractor, self).__init__()\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(2048, 128)\n\n    def forward(self, x, coordinates):\n        batch_size, _, _, _ = x.size()\n        local_embeddings = []\n        for i in range(batch_size):\n            patches = []\n            for (top_left, bottom_right) in coordinates[i]:\n                patch = x[i:i+1, :, top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n                patches.append(patch)\n            if len(patches) == 0:\n                continue\n            patches = torch.cat(patches, dim=0)\n            features = self.resnet(patches)\n            pooled_features = self.avg_pool(features)\n            pooled_features = pooled_features.view(pooled_features.size(0), -1)\n            embeddings = self.fc(pooled_features)\n            local_embeddings.append(embeddings)\n        local_embeddings = torch.stack(local_embeddings)\n        return local_embeddings\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.fc = nn.Linear(d_model, d_model)\n\n    def forward(self, Q, K, V):\n        batch_size = Q.size(0)\n        Q = self.Wq(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.Wk(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.Wv(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n        attention = F.softmax(scores, dim=-1)\n        context = torch.matmul(attention, V).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.fc(context)\n        return output\n\nclass AFFM(nn.Module):\n    def __init__(self, global_dim, local_dim, num_heads):\n        super(AFFM, self).__init__()\n        self.multihead_attention = MultiHeadAttention(global_dim + local_dim, num_heads)\n        self.classifier = nn.Linear(global_dim + local_dim, 1)  # Assuming binary classification\n\n    def forward(self, global_embeddings, local_embeddings):\n        embeddings = torch.cat((global_embeddings, local_embeddings), dim=1)\n        fused_features = self.multihead_attention(embeddings, embeddings, embeddings)\n        output = self.classifier(fused_features)\n        return output\n\nclass FullArchitecture(nn.Module):\n    def __init__(self):\n        super(FullArchitecture, self).__init__()\n        self.global_branch = GlobalBranch()\n        self.patch_selection_module = PatchSelectionModule()\n        self.local_embedding_extractor = LocalEmbeddingExtractor()\n        self.affm = AFFM(global_dim=128, local_dim=128, num_heads=8)\n\n    def forward(self, x):\n        original_image_size = x.shape[2:]\n        F, global_embeddings = self.global_branch(x)\n        patch_coordinates = self.patch_selection_module(F, original_image_size)\n        local_embeddings = self.local_embedding_extractor(x, patch_coordinates)\n        output = self.affm(global_embeddings, local_embeddings)\n        return output\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs('/kaggle/working/dataset/train/real', exist_ok=True)\nos.makedirs('/kaggle/working/dataset/train/fake', exist_ok=True)\nos.makedirs('/kaggle/working/dataset/val/real', exist_ok=True)\nos.makedirs('/kaggle/working/dataset/val/fake', exist_ok=True)\nos.makedirs('/kaggle/working/dataset/test/real', exist_ok=True)\nos.makedirs('/kaggle/working/dataset/test/fake', exist_ok=True)","metadata":{"_uuid":"7de2b0ad-49cb-4ec2-ac69-b75ebca7e753","_cell_guid":"5a55cd87-b77f-4779-a7ea-9f1d702ab267","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:48.203501Z","iopub.execute_input":"2024-07-24T10:11:48.203855Z","iopub.status.idle":"2024-07-24T10:11:48.209452Z","shell.execute_reply.started":"2024-07-24T10:11:48.203829Z","shell.execute_reply":"2024-07-24T10:11:48.208676Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for file in real_train:\n    shutil.copy(os.path.join(real_images_path, file), '/kaggle/working/dataset/train/real')\n\nfor file in fake_train:\n    shutil.copy(os.path.join(fake_images_path, file), '/kaggle/working/dataset/train/fake')\n\nfor file in real_val:\n    shutil.copy(os.path.join(real_images_path, file), '/kaggle/working/dataset/val/real')\n\nfor file in fake_val:\n    shutil.copy(os.path.join(fake_images_path, file), '/kaggle/working/dataset/val/fake')\n\nfor file in real_test:\n    shutil.copy(os.path.join(real_images_path, file), '/kaggle/working/dataset/test/real')\n\nfor file in fake_test:\n    shutil.copy(os.path.join(fake_images_path, file), '/kaggle/working/dataset/test/fake')","metadata":{"_uuid":"c35354d3-8215-4240-8280-fc2e51cd6491","_cell_guid":"86db18ea-7997-425d-ad69-c273d6f349e7","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:50.282219Z","iopub.execute_input":"2024-07-24T10:11:50.282545Z","iopub.status.idle":"2024-07-24T10:11:55.430606Z","shell.execute_reply.started":"2024-07-24T10:11:50.282518Z","shell.execute_reply":"2024-07-24T10:11:55.429717Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100\nlr = 0.003\nbatch_size = 32","metadata":{"_uuid":"8c26f81d-886d-4b82-a79e-d4d2e70da8ce","_cell_guid":"2a75cf88-e26a-470c-969b-677724bbecd4","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:55.432104Z","iopub.execute_input":"2024-07-24T10:11:55.432356Z","iopub.status.idle":"2024-07-24T10:11:55.436117Z","shell.execute_reply.started":"2024-07-24T10:11:55.432332Z","shell.execute_reply":"2024-07-24T10:11:55.435337Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_dir = '/kaggle/working/dataset/train'\nval_dir = '/kaggle/working/dataset/val'\ntest_dir = '/kaggle/working/dataset/test'\n\n\n\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\nval_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f'Training samples: {len(train_dataset)}')\nprint(f'Testing samples: {len(test_dataset)}')","metadata":{"_uuid":"507440ec-6dfd-4756-b1ab-93103e6c896b","_cell_guid":"8a578148-a83a-4205-92d5-df1cd3122daf","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:55.437211Z","iopub.execute_input":"2024-07-24T10:11:55.437446Z","iopub.status.idle":"2024-07-24T10:11:55.452152Z","shell.execute_reply.started":"2024-07-24T10:11:55.437422Z","shell.execute_reply":"2024-07-24T10:11:55.451367Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"8c8428e9-2b64-4c09-9dc0-6a7ed3c7f6be","_cell_guid":"6a82bc9a-371d-426d-acd5-a9862d706f1e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\n# Define device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass PatchSelectionModule(nn.Module):\n    def __init__(self, patch_size=8, num_patches=0.75):\n        super(PatchSelectionModule, self).__init__()\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n    def calculate_patch_scores(self, activation_map):\n        # Get dimensions\n        batch_size, _, height, width = activation_map.size()\n        scores = F.avg_pool2d(activation_map, self.patch_size, stride=2)\n        return scores\n\n    def non_max_suppression(self, coords, scores, threshold=0.5):\n        sorted_indices = torch.argsort(scores, descending=True)\n        sorted_scores = scores[sorted_indices]\n        coords = torch.tensor(coords)\n        sorted_coords = coords[sorted_indices]\n\n        keep_indices = []\n        while len(sorted_coords) > 0:\n            current = sorted_coords[0]\n            keep_indices.append(sorted_indices[0])\n            sorted_coords = sorted_coords[1:]\n            sorted_indices = sorted_indices[1:]\n            if len(sorted_coords) == 0:\n                break\n            ious = self.iou(current, sorted_coords)\n            keep = ious < threshold\n            sorted_coords = sorted_coords[keep]\n            sorted_indices = sorted_indices[keep]\n        return coords[keep_indices], scores[keep_indices]\n\n    def iou(self, boxA, boxesB):\n        xA1, yA1 = boxA[0]\n        xA2, yA2 = boxA[1]\n        xB1, yB1 = boxesB[:, 0, 0], boxesB[:, 0, 1]\n        xB2, yB2 = boxesB[:, 1, 0], boxesB[:, 1, 1]\n        inter_x1 = torch.max(xA1, xB1)\n        inter_y1 = torch.max(yA1, yB1)\n        inter_x2 = torch.min(xA2, xB2)\n        inter_y2 = torch.min(yA2, yB2)\n        interArea = torch.max(torch.tensor(0), inter_x2 - inter_x1) * torch.max(torch.tensor(0), inter_y2 - inter_y1)\n        boxAArea = (xA2 - xA1) * (yA2 - yA1)\n        boxBArea = (xB2 - xB1) * (yB2 - yB1)\n        iou = interArea / (boxAArea + boxBArea - interArea)\n        return iou\n\n    def calc_topk_coords(self, scores, threshold=0.5):\n        flattened_scores = scores.view(scores.size(0), -1)\n        num_patches = min(self.num_patches * flattened_scores.shape[1], flattened_scores.shape[1])\n        topk_scores, topk_indices = torch.topk(flattened_scores, int(num_patches), dim=1)\n        topk_coords = []\n        for idx in range(scores.size(0)):\n            coords = []\n            for index in topk_indices[idx]:\n                x, y = divmod(index.item(), scores.size(2))\n                coords.append((x, y))\n            topk_coords.append(coords)\n        return topk_scores, topk_coords\n\n    def get_patch_coordinates(self, activation_map, original_image_size):\n        scores = self.calculate_patch_scores(activation_map)\n        topk_scores, topk_coords = self.calc_topk_coords(scores)\n        scale_h = original_image_size[0] / activation_map.size(2)\n        scale_w = original_image_size[1] / activation_map.size(3)\n        patch_coords = []\n        for coords in topk_coords:\n            image_coords = []\n            for (x, y) in coords:\n                top_left = (int(y * scale_w), int(x * scale_h))\n                bottom_right = (int((y + self.patch_size) * scale_w), int((x + self.patch_size) * scale_h))\n                image_coords.append((top_left, bottom_right))\n            patch_coords.append(image_coords)\n        return patch_coords\n\n    def get_patches(self, patch_coordinates, original_image):\n        patches = []\n        for coords in patch_coordinates:\n            image_patches = []\n            for (top_left, bottom_right) in coords:\n                patch = original_image[:, top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n                image_patches.append(patch)\n            patches.append(torch.stack(image_patches))\n        patches = torch.stack(patches)\n        return patches\n\n    def forward(self, activation_map, original_image_size):\n        activation_map = activation_map.sum(dim=1).unsqueeze(1)\n        patch_coordinates = self.get_patch_coordinates(activation_map, original_image_size)\n        return patch_coordinates\n\nclass GlobalBranch(nn.Module):\n    def __init__(self):\n        super(GlobalBranch, self).__init__()\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(2048, 128)\n\n    def forward(self, x):\n        F = self.resnet(x)\n        pooled_F = self.avg_pool(F)\n        pooled_F = pooled_F.view(pooled_F.size(0), -1)\n        global_embeddings = self.fc(pooled_F)\n        return F, global_embeddings\n\nclass LocalEmbeddingExtractor(nn.Module):\n    def __init__(self):\n        super(LocalEmbeddingExtractor, self).__init__()\n        self.resnet = models.resnet50(pretrained=True)\n        self.resnet = nn.Sequential(*list(self.resnet.children())[:-2])\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(2048, 128)\n\n    def forward(self, x, coordinates):\n        batch_size, _, _, _ = x.size()\n        local_embeddings = []\n        for i in range(batch_size):\n            patches = []\n            for (top_left, bottom_right) in coordinates[i]:\n                patch = x[i:i+1, :, top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n                patches.append(patch)\n            if len(patches) == 0:\n                continue\n            patches = torch.cat(patches, dim=0)\n            features = self.resnet(patches)\n            pooled_features = self.avg_pool(features)\n            pooled_features = pooled_features.view(pooled_features.size(0), -1)\n            embeddings = self.fc(pooled_features)\n            local_embeddings.append(embeddings)\n        local_embeddings = torch.stack(local_embeddings)\n        return local_embeddings\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        self.Wq = nn.Linear(d_model, d_model)\n        self.Wk = nn.Linear(d_model, d_model)\n        self.Wv = nn.Linear(d_model, d_model)\n        self.fc = nn.Linear(d_model, d_model)\n\n    def forward(self, Q, K, V):\n        batch_size = Q.size(0)\n        Q = self.Wq(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = self.Wk(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = self.Wv(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32, device=Q.device))\n        attention = F.softmax(scores, dim=-1)\n        context = torch.matmul(attention, V).transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n        output = self.fc(context)\n        return output\n\nclass AFFM(nn.Module):\n    def __init__(self, global_dim, local_dim, num_heads):\n        super(AFFM, self).__init__()\n        self.multihead_attention = MultiHeadAttention(global_dim + local_dim, num_heads)\n        self.classifier = nn.Linear(global_dim + local_dim, 1)  # Assuming binary classification\n\n    def forward(self, global_embeddings, local_embeddings):\n        embeddings = torch.cat((global_embeddings, local_embeddings.mean(dim=1)), dim=1)\n        fused_features = self.multihead_attention(embeddings, embeddings, embeddings)\n        output = self.classifier(fused_features)\n        return output\n\nclass FullArchitecture(nn.Module):\n    def __init__(self):\n        super(FullArchitecture, self).__init__()\n        self.global_branch = GlobalBranch()\n        self.patch_selection_module = PatchSelectionModule()\n        self.local_embedding_extractor = LocalEmbeddingExtractor()\n        self.affm = AFFM(global_dim=128, local_dim=128, num_heads=8)\n\n    def forward(self, x):\n        original_image_size = x.shape[2:]\n        F, global_embeddings = self.global_branch(x)\n        patch_coordinates = self.patch_selection_module(F, original_image_size)\n        local_embeddings = self.local_embedding_extractor(x, patch_coordinates)\n        output = self.affm(global_embeddings, local_embeddings)\n        return output\n\n# Example usage\n# Assuming the input tensor 'images' has shape (B, C, H, W)\n# images = torch.randn((3, 3, 512, 512)).to(device)  # Example batch of 2 images\n# model = FullArchitecture().to(device)\n# output = model(images)\n# print(output)","metadata":{"_uuid":"25dcd248-4139-41af-96a8-a13cbedc8c5d","_cell_guid":"f5ea0e0d-7a01-4d9a-b632-35e1b6a53672","execution":{"iopub.status.busy":"2024-07-24T10:11:56.898873Z","iopub.execute_input":"2024-07-24T10:11:56.899205Z","iopub.status.idle":"2024-07-24T10:11:56.931589Z","shell.execute_reply.started":"2024-07-24T10:11:56.899178Z","shell.execute_reply":"2024-07-24T10:11:56.930742Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\nmodel = FullArchitecture().to(device)\n\n# Define loss function and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)","metadata":{"_uuid":"d152c733-d2bb-4a39-bfd1-390d4efb68c3","_cell_guid":"3f7d2a8d-d0d8-4c39-9467-afba4c6b9a08","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:11:57.809554Z","iopub.execute_input":"2024-07-24T10:11:57.809898Z","iopub.status.idle":"2024-07-24T10:11:59.426356Z","shell.execute_reply.started":"2024-07-24T10:11:57.809872Z","shell.execute_reply":"2024-07-24T10:11:59.425345Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device).float()\n        \n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        outputs = outputs.squeeze()  # Remove extra dimension if necessary\n        \n        # Calculate loss\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    epoch_loss = running_loss / len(train_loader)\n    print(f\"Training Loss: {epoch_loss:.4f}\")\n\ndef test(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device).float()\n            \n            # Forward pass\n            outputs = model(images)\n            outputs = outputs.squeeze()  # Remove extra dimension if necessary\n            \n            # Calculate loss\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            \n            # Calculate accuracy\n            predicted = torch.round(torch.sigmoid(outputs))\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    epoch_loss = running_loss / len(test_loader)\n    accuracy = correct / total\n    print(f\"Test Loss: {epoch_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n\n# Example usage:\n# Assuming `train_loader` and `test_loader` are DataLoader objects\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    train(model, train_loader, criterion, optimizer, device)\n    test(model, test_loader, criterion, device)","metadata":{"_uuid":"1de4fd6c-9e3c-4d28-a410-503e6f404854","_cell_guid":"abe6b9bf-4b2a-4f92-b6cf-048a581c640a","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:12:02.808762Z","iopub.execute_input":"2024-07-24T10:12:02.809780Z","iopub.status.idle":"2024-07-24T10:36:09.724858Z","shell.execute_reply.started":"2024-07-24T10:12:02.809744Z","shell.execute_reply":"2024-07-24T10:36:09.723665Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime as dt\ndef save_model(model, model_name,model_evaluation_history):\n    model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n    # Define the string date format.\n    # Get the current Date and Time in a DateTime Object.\n    # Convert the DateTime object to string according to the style mentioned in date_time_format string.\n    date_time_format = '%Y_%m_%d__%H_%M_%S'\n    current_date_time_dt = dt.datetime.now()\n    current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n\n    # Define a useful name for our model to make it easy for us while navigating through multiple saved models.\n    model_file_name = f'{model_name}___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.pth'\n\n    # Save your Model.\n    torch.save(model.state_dict(), f'{model_file_name}_entire_dict')\n    torch.save(model, f'{model_file_name}_entire')","metadata":{"_uuid":"79cc0d0e-b810-4c34-9179-56317385a448","_cell_guid":"3a5c42df-57a9-40a9-ab11-f5ce235a4d1c","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:36:09.726782Z","iopub.execute_input":"2024-07-24T10:36:09.727080Z","iopub.status.idle":"2024-07-24T10:36:09.734003Z","shell.execute_reply.started":"2024-07-24T10:36:09.727050Z","shell.execute_reply":"2024-07-24T10:36:09.732551Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_model(model, \"Fusion\", (0, 0))","metadata":{"_uuid":"ab68ac3a-8265-4bc2-96d7-90e212ae75ba","_cell_guid":"42cef989-ac5f-4183-855b-1803d8798beb","collapsed":false,"execution":{"iopub.status.busy":"2024-07-24T10:36:09.735086Z","iopub.execute_input":"2024-07-24T10:36:09.735364Z","iopub.status.idle":"2024-07-24T10:36:10.368978Z","shell.execute_reply.started":"2024-07-24T10:36:09.735337Z","shell.execute_reply":"2024-07-24T10:36:10.368051Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_uuid":"4d41e4a9-1f67-433b-8eea-1e95281c0aad","_cell_guid":"c4cbd99e-54c2-4990-be1b-9962c226671f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}